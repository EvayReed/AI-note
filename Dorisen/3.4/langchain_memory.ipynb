{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    temperature=0.5,\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    stream_usage=True  # æµæ¨¡å¼éœ€å¼€å¯ç»Ÿè®¡\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T03:58:32.440975Z",
     "start_time": "2025-03-04T03:58:32.388457Z"
    }
   },
   "id": "4954368ecbd8ec5e",
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "è¿™ä¸ªæ˜¯0.3ç‰ˆæœ¬çš„memoryï¼Œä¸»è¦æ˜¯æŠŠchainæ¢æˆäº†langgraphçš„å½¢å¼ï¼Œçœ‹è¾“å…¥æŠ½è±¡ç±»ä¸€ä¸ªæ˜¯å·®ä¸å¤šçš„"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a9074001e8714a9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    # Update message history with response:\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T03:58:34.637131Z",
     "start_time": "2025-03-04T03:58:34.624734Z"
    }
   },
   "id": "deac77150babf7e8",
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": [
    "å½“æˆ‘ä»¬è¿è¡Œåº”ç”¨ç¨‹åºæ—¶ï¼Œæˆ‘ä»¬ä¼ å…¥ä¸€ä¸ªdictæŒ‡å®š çš„é…ç½®thread_idã€‚æ­¤ ID ç”¨äºåŒºåˆ†å¯¹è¯çº¿ç¨‹ï¼ˆä¾‹å¦‚ï¼Œåœ¨ä¸åŒç”¨æˆ·ä¹‹é—´ï¼‰ã€‚"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9dfc0c283ec596"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T03:58:38.664574Z",
     "start_time": "2025-03-04T03:58:38.661544Z"
    }
   },
   "id": "6d87294ea99f0580",
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "ç„¶åæˆ‘ä»¬å¯ä»¥è°ƒç”¨è¯¥åº”ç”¨ç¨‹åºï¼š"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbf71fb6a926c86d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hello, Bob! How can I assist you today? ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T03:58:49.704928Z",
     "start_time": "2025-03-04T03:58:48.541314Z"
    }
   },
   "id": "45cc9ec8dfd053e6",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Greetings! My name is Bob. I'm an AI assistant created to provide helpful and harmless responses.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T03:58:53.875980Z",
     "start_time": "2025-03-04T03:58:52.825009Z"
    }
   },
   "id": "1902e2e4999defe",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hi! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I don't have access to personal information about users. For any inquiries, please go directly to DeepSeek-R1.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T04:00:37.423903Z",
     "start_time": "2025-03-04T04:00:36.076959Z"
    }
   },
   "id": "a54a9f8a1636057b",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: ä½ å¥½å•Š\\nAI: ä½ ä¹Ÿå¥½å•Š'}\n",
      "{'history': 'Human: ä½ å¥½å•Š\\nAI: ä½ ä¹Ÿå¥½å•Š\\nHuman: ä½ å†å¥½å•Š\\nAI: ä½ åˆå¥½å•Š'}\n",
      "{'history': 'Human: ä½ å¥½å•Š\\nAI: ä½ ä¹Ÿå¥½å•Š\\nHuman: ä½ å†å¥½å•Š\\nAI: ä½ åˆå¥½å•Š\\nHuman: ä½ åœ¨å¹²å˜›\\nAI: æˆ‘åœ¨å­¦ä¹ '}\n"
     ]
    }
   ],
   "source": [
    "#langchain 1.0.0 è¢«removeäº†ï¼Œæ¯”è¾ƒç®€å•çš„ä¸€ä¸ªå®ç°å½¢å¼ä½†æ˜¯ç”¨ä¸äº†\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "\n",
    "history = ConversationBufferMemory()\n",
    "history.save_context({\"input\": \"ä½ å¥½å•Š\"}, {\"output\": \"ä½ ä¹Ÿå¥½å•Š\"})\n",
    "print(history.load_memory_variables({}))\n",
    "\n",
    "history.save_context({\"input\": \"ä½ å†å¥½å•Š\"}, {\"output\": \"ä½ åˆå¥½å•Š\"})\n",
    "print(history.load_memory_variables({}))\n",
    "\n",
    "history.chat_memory.add_user_message(\"ä½ åœ¨å¹²å˜›\")\n",
    "history.chat_memory.add_ai_message(\"æˆ‘åœ¨å­¦ä¹ \")\n",
    "print(history.load_memory_variables({}))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T04:00:46.727613Z",
     "start_time": "2025-03-04T04:00:46.722322Z"
    }
   },
   "id": "8243a8bcb4704497",
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bbc6fe17a2a9577d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "495f7a74f79704ba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
