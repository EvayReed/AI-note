{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:06:40.691304Z",
     "start_time": "2025-02-25T09:06:23.740033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "响应内容: <think>\n",
      "嗯，用户问的是“望梅止渴是什么意思”，首先我要确认这个词组的准确解释。我记得“望梅止渴”出自苏轼的名句，是关于如何应对内心的焦虑和恐惧的情感表达。\n",
      "\n",
      "好的，那么“望”在这里就是希望，“梅”是指梅树，而“竭渴”则表示已经无止境地渴望。所以整体来说，这句话是在说如果能够以一种积极的态度去看待或面对自己的情绪，就能避免陷入停滞不前的情况。\n",
      "\n",
      "接下来，我需要解释得更详细一点，可能包括它的文化背景、历史来源以及后来的演变情况。比如，原句中的梅树象征着坚韧和坚持，后来被用来比喻遇到困难时要保持乐观的心态。\n",
      "\n",
      "另外，我还应该说明这个表达在现代社会的应用范围，不仅仅是个人情感，还可以用于职场、学习等领域，帮助人们在面对各种压力或挑战时，能够保持冷静和积极的态度。\n",
      "\n",
      "最后，确保用词准确，避免误解。比如“望梅止渴”并不是说去望别人的东西，而是指自己希望通过某种方式来解决内心的困扰。这样用户就能更清楚地理解这句话的含义了。\n",
      "</think>\n",
      "\n",
      "“望梅止渴”出自宋代文学家苏轼的名句，表达了一种积极乐观的生活态度。具体意思是：如果能够以一种积极的态度去看待或面对自己的情绪，就能够避免陷入停滞不前的状态。\n",
      "\n",
      "### 意义解析：\n",
      "1. **希望与解决**：句子中的“望梅”指希望、愿望，“竭渴”则表示已经无止境地渴望。整体表达的是通过积极的行动和态度来解决问题，最终达到目标或达成平衡。\n",
      "   \n",
      "2. **文化背景与历史来源**：\n",
      "   - 这句话出自苏轼的名句《定风波·望梅止渴》，展现了苏轼深谙人生哲理的精神境界。\n",
      "   - 原句反映了苏轼在面对逆境时保持冷静、积极的心态，通过努力和努力，最终找到解决办法。\n",
      "\n",
      "3. **现代应用**：\n",
      "   - “望梅止渴”不仅限于个人情感，也可以用于形容在面对困难或压力时，能够保持乐观、积极的态度。\n",
      "   - 它被广泛应用于职场、学习、感情等各个领域，帮助人们在挑战面前保持冷静、前进。\n",
      "\n",
      "4. **文化意义**：\n",
      "   - 这句话体现了苏轼的高远学风和对生活的真实理解，提醒人们不要因为焦虑而放弃自我，而是要通过积极的态度去应对。\n",
      "\n",
      "总之，“望梅止渴”是一种鼓励人们以积极主动的态度面对困难和挑战，最终实现目标或达成平衡的精神状态。\n",
      "输入Token: 9\n",
      "输出Token: 559\n",
      "总消耗Token: 568\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    temperature=0.7,\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    stream_usage=True  # 流模式需开启统计\n",
    ")\n",
    "\n",
    "response = llm.invoke([\n",
    "    HumanMessage(content=\"望梅止渴是什么意思\")\n",
    "])\n",
    "\n",
    "# 输出内容和token统计\n",
    "print(f\"响应内容: {response.content}\")\n",
    "print(f\"输入Token: {response.usage_metadata['input_tokens']}\")\n",
    "print(f\"输出Token: {response.usage_metadata['output_tokens']}\")\n",
    "print(f\"总消耗Token: {response.usage_metadata['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e6283f5e041991d4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你的名字叫做张三,你的年龄是18\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"你的名字叫做{name},你的年龄是{age}\")\n",
    "\n",
    "result = prompt.format(name=\"张三\", age=18)\n",
    "\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:08:00.324552Z",
     "start_time": "2025-02-25T09:08:00.313012Z"
    }
   },
   "id": "27c1203f7368238c",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='你是一个程序员,你的名字叫张三', additional_kwargs={}, response_metadata={}), HumanMessage(content='你好', additional_kwargs={}, response_metadata={}), AIMessage(content='你好,我叫张三', additional_kwargs={}, response_metadata={}), HumanMessage(content='今天天气怎么样？', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"你是一个程序员,你的名字叫{name}\"),\n",
    "    (\"human\", \"你好\"),\n",
    "    (\"ai\", \"你好,我叫{name}\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "message = prompt_template.format_messages(name=\"张三\", user_input=\"今天天气怎么样？\")\n",
    "print(message)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:09:03.099848Z",
     "start_time": "2025-02-25T09:09:03.093742Z"
    }
   },
   "id": "a54f5ae9337e2a1b",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='你好，我是张三，请问今天天气怎么样？', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"你好，我是{name}，请问{user_input}\"\n",
    ")\n",
    "message = prompt_template.format_messages(name=\"张三\", user_input=\"今天天气怎么样？\")\n",
    "print(message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:09:23.733072Z",
     "start_time": "2025-02-25T09:09:23.728795Z"
    }
   },
   "id": "2edb26a83b7e0914",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='你是一个搞笑风趣的智能助手', additional_kwargs={}, response_metadata={}), SystemMessage(content='你的名字叫做张三,请根据用户的问题回答。', additional_kwargs={}, response_metadata={}), HumanMessage(content='用户的问题是：今天天气怎么样？', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(\n",
    "        content=(\n",
    "            \"你是一个搞笑风趣的智能助手\"\n",
    "        )\n",
    "    ),\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"你的名字叫做{name},请根据用户的问题回答。\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"用户的问题是：{user_input}\"\n",
    "    ),\n",
    "])\n",
    "message = chat_template.format_messages(name=\"张三\", user_input=\"今天天气怎么样？\")\n",
    "print(message)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:14:00.004864Z",
     "start_time": "2025-02-25T09:14:00.000058Z"
    }
   },
   "id": "10f01ca644980fe9",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='你是一个起名大师', additional_kwargs={}, response_metadata={}, assitional_kwargs={'大师名字': '王麻子'}), HumanMessage(content='请问大师叫什么', additional_kwargs={}, response_metadata={}), AIMessage(content='我叫王麻子', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import AIMessage\n",
    "\n",
    "\n",
    "sy = SystemMessage(\n",
    "\tcontent=\"你是一个起名大师\",\n",
    "  assitional_kwargs={\"大师名字\":\"王麻子\"}\n",
    ")\n",
    "\n",
    "hu = HumanMessage(\n",
    "\tcontent=\"请问大师叫什么\"\n",
    ")\n",
    "\n",
    "ai = AIMessage(\n",
    "\tcontent=\"我叫王麻子\"\n",
    ")\n",
    "\n",
    "print([sy, hu, ai])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:15:17.084317Z",
     "start_time": "2025-02-25T09:15:17.078931Z"
    }
   },
   "id": "bb40f5dcb645ef09",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to understand this Python function called hello_word. The function takes an argument named address and does two things: it prints a message that's the combination of \"Hello, world!\" and the address, and then it returns another string that starts with \"Hello, \", followed by the address in parentheses.\n",
      "\n",
      "First, let me look at how to call this function. If I want to use hello_word with a specific address, say \"123\", I would write something like:\n",
      "\n",
      "hello_word(\"123\")\n",
      "\n",
      "When I run that, the function should print \"Hello, world! 123\" and return the string \"Hello, 123!\".\n",
      "\n",
      "Wait, let me test this. If I call hello_word with an empty string as the address, it would print \"Hello, world!\" and return \"Hello, \"! That makes sense because the function uses f-strings to include the address in both actions.\n",
      "\n",
      "What if the address is None? The function should still work because Python allows using None in strings. So calling hello_word(None) would result in \"Hello, world! None\" being printed and the returned string would be \"Hello, None!\".\n",
      "\n",
      "I'm thinking about edge cases. What if the address is not a string? Like an integer or another type? The function expects an address, so maybe it should handle that by converting it to a string. But in the current code, it doesn't, which might cause errors if someone passes something else. I wonder if there's a way to make it more robust.\n",
      "\n",
      "Also, I'm curious about whether this function is intended for user input or if the address should come from an external source. If it's supposed to take user input, maybe I should prompt the user for the address before processing it. But since the source code doesn't include any prompts, perhaps that's beyond the scope of what's being asked.\n",
      "\n",
      "Another thing to consider is whether this function uses exceptions or error handling. For example, if the address isn't a number or string, trying to print or return might cause issues. Maybe adding try-except blocks would make it more robust, but again, unless specified, perhaps that's not necessary right now.\n",
      "\n",
      "I should also think about how this function relates to other similar functions. Like, there's print(\"Hello, world\") which just outputs the string without any input, and return \"Hello, world!\" on its own. This hello_word function adds an address parameter so it can be used in more complex ways where the user might want different messages based on what they enter.\n",
      "\n",
      "So, to sum up, the function takes an address, combines it with a fixed message, prints it, and returns the combined string. It handles various data types gracefully but could benefit from better input validation if needed.\n",
      "</think>\n",
      "\n",
      "The `hello_word` function is designed to take an address as input and return a formatted string. Here's how it works:\n",
      "\n",
      "1. **Parameters**: The function takes one parameter, `address`, which can be any type (string, integer, etc.).\n",
      "\n",
      "2. **Printing the Message**:\n",
      "   ```python\n",
      "   print(\"Hello, world!\" + address)\n",
      "   ```\n",
      "   When an address is provided, this line prints a message that combines \"Hello, world!\" with the given address.\n",
      "\n",
      "3. **Returning the String**: \n",
      "   ```python\n",
      "   return f\"Hello, {address}!\"\n",
      "   ```\n",
      "   The function returns another string that starts with \"Hello, \", followed by the address in parentheses.\n",
      "\n",
      "**Examples**:\n",
      "- `hello_word(\"123\")` results in:\n",
      "  - Printed message: \"Hello, world! 123\"\n",
      "  - Returned string: \"Hello, 123!\"\n",
      "  \n",
      "- `hello_word(None)` results in:\n",
      "  - Printed message: \"Hello, world! None\"\n",
      "  - returned string: \"Hello, None!\"\n",
      "\n",
      "**Considerations**: The function assumes the address is a valid type. If not, it might cause errors when used outside its intended scope. For robustness, error handling could be added to handle unexpected input types.\n",
      "\n",
      "This function is versatile for scenarios where user input or specific messages need to be combined with predefined strings based on an address parameter.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import StringPromptTemplate\n",
    "\n",
    "\n",
    "def hello_word(address):\n",
    "    print(\"Hello, world!\" + address)\n",
    "    return f\"Hello, {address}!\"\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"\\\n",
    "    You are a helpful assistant that answers questions based on the provided context.\n",
    "    function name: {function_name}\n",
    "    source code:\n",
    "    {source_code}\n",
    "    explain:\n",
    "\"\"\"\n",
    "\n",
    "import inspect  # 这个包可以根据函数名，获取到函数源代码\n",
    "\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    # 获取源代码\n",
    "    return inspect.getsource(function_name)\n",
    "\n",
    "\n",
    "class CustmPrompt(StringPromptTemplate):\n",
    "    def format(self, **kwargs) -> str:\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        prompt = PROMPT.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__, source_code=source_code\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "\n",
    "a = CustmPrompt(input_variables=[\"function_name\"])\n",
    "pm = a.format(function_name=hello_word)\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    temperature=0.7,\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "result = llm.invoke(pm)\n",
    "print(result.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:16:49.799947Z",
     "start_time": "2025-02-25T09:16:23.877827Z"
    }
   },
   "id": "93c1c8f9b7e98d2d",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是科学家, 你有着严谨的品质\n",
      "你会遵从以下行为：\n",
      "1. 确保实验结果准确\n",
      "2. 记录实验过程\n",
      "3. 分析实验数据\n",
      "你不能遵从以下行为：\n",
      "1. 不做实验\n",
      "2. 不记录实验过程\n",
      "3. 不分析实验数据\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate, PipelinePromptTemplate\n",
    "\n",
    "full_template = \"\"\"{Character}\n",
    "{Behavior}\n",
    "{Prohibit}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "\n",
    "Character_template = \"你是{person}, 你有着{attribute}的品质\"\n",
    "Character_prompt = PromptTemplate.from_template(\n",
    "    Character_template\n",
    ")\n",
    "\n",
    "behavior_template = \"你会遵从以下行为：\\n{behavior_list}\"\n",
    "behavior_prompt = PromptTemplate.from_template(behavior_template)\n",
    "\n",
    "prohibit_template = \"你不能遵从以下行为：\\n{prohibit_list}\"\n",
    "prohibit_prompt = PromptTemplate.from_template(prohibit_template)\n",
    "\n",
    "input_prompts = [\n",
    "    (\"Character\", Character_prompt),\n",
    "    (\"Behavior\", behavior_prompt),\n",
    "    (\"Prohibit\", prohibit_prompt),\n",
    "]\n",
    "\n",
    "pipeline_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=full_prompt,\n",
    "    pipeline_prompts=input_prompts\n",
    ")\n",
    "\n",
    "print(pipeline_prompt.format(\n",
    "    person=\"科学家\",\n",
    "    attribute=\"严谨\",\n",
    "    behavior_list=\"1. 确保实验结果准确\\n2. 记录实验过程\\n3. 分析实验数据\",\n",
    "    prohibit_list=\"1. 不做实验\\n2. 不记录实验过程\\n3. 不分析实验数据\"\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:19:14.808289Z",
     "start_time": "2025-02-25T09:19:14.802230Z"
    }
   },
   "id": "99b6198b992ba2d",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给我讲一个关于科学家的热爱搞发明的故事\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"simple_prompt.json\")\n",
    "print(prompt.format(name=\"科学家\", what=\"热爱搞发明\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:27:32.522107Z",
     "start_time": "2025-02-25T09:27:32.518332Z"
    }
   },
   "id": "986694065c8fb05d",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:30:43.803701Z",
     "start_time": "2025-02-25T09:30:43.794827Z"
    }
   },
   "id": "96695e6876470e7a",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "给出每个输入词的反义词\n",
      "\n",
      "原词：happy\n",
      " 反义：sad\n",
      "\n",
      "原词：tall\n",
      " 反义：short\n",
      "\n",
      "原词：sunny\n",
      " 反义：rainy\n",
      "\n",
      "原词：windy\n",
      " 反义：calm\n",
      "\n",
      "原词：hot\n",
      " 反义：cold\n",
      "\n",
      "原词：fast\n",
      " 反义：slow\n",
      "\n",
      "原词：forward\n",
      "反义:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# 假设已经有这么多的提示词示例组\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"rainy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "    {\"input\": \"hot\", \"output\": \"cold\"},\n",
    "    {\"input\": \"fast\", \"output\": \"slow\"},\n",
    "    {\"input\": \"big\", \"output\": \"small\"},\n",
    "    {\"input\": \"bright\", \"output\": \"dark\"},\n",
    "    {\"input\": \"strong\", \"output\": \"weak\"},\n",
    "    {\"input\": \"clean\", \"output\": \"dirty\"},\n",
    "    {\"input\": \"heavy\", \"output\": \"light\"},\n",
    "    {\"input\": \"happy\", \"output\": \"angry\"},\n",
    "    {\"input\": \"high\", \"output\": \"low\"},\n",
    "    {\"input\": \"rich\", \"output\": \"poor\"},\n",
    "    {\"input\": \"beautiful\", \"output\": \"ugly\"},\n",
    "    {\"input\": \"full\", \"output\": \"empty\"},\n",
    "    {\"input\": \"young\", \"output\": \"old\"},\n",
    "    {\"input\": \"loud\", \"output\": \"quiet\"},\n",
    "    {\"input\": \"soft\", \"output\": \"hard\"},\n",
    "    {\"input\": \"strong\", \"output\": \"fragile\"},\n",
    "    {\"input\": \"sweet\", \"output\": \"sour\"},\n",
    "    {\"input\": \"clean\", \"output\": \"messy\"},\n",
    "    {\"input\": \"open\", \"output\": \"closed\"},\n",
    "    {\"input\": \"warm\", \"output\": \"cool\"}\n",
    "]\n",
    "\n",
    "# 构造提示词模板\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"原词：{input}\\n 反义：{output}\"\n",
    ")\n",
    "\n",
    "# 调用长度示例选择器\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,  # 传入示例提示词组\n",
    "    example_prompt=example_prompt,  # 传入的提示词模板\n",
    "    max_length=20  # 格式化后，的提示词的最大长度\n",
    ")\n",
    "\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"给出每个输入词的反义词\",\n",
    "    suffix=\"原词：{adjective}\\n反义:\",\n",
    "    input_variables=[\"adjective\"]\n",
    ")\n",
    "\n",
    "print(dynamic_prompt.format(adjective=\"forward\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:42:04.254586Z",
     "start_time": "2025-02-25T09:42:04.243823Z"
    }
   },
   "id": "a3086bb5f90e5c06",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请根据以下示例生成反义词：\n",
      "\n",
      "原词：big\n",
      "反义：small\n",
      "\n",
      "原词：轻松\n",
      "反义：紧张\n",
      "\n",
      "原词是：worried\n",
      "反义词：\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import MaxMarginalRelevanceExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import FewShotPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 假设已经有这么多的提示词示例组\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"rainy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "    {\"input\": \"hot\", \"output\": \"cold\"},\n",
    "    {\"input\": \"fast\", \"output\": \"slow\"},\n",
    "    {\"input\": \"big\", \"output\": \"small\"},\n",
    "    {\"input\": \"bright\", \"output\": \"dark\"},\n",
    "    {\"input\": \"strong\", \"output\": \"weak\"},\n",
    "    {\"input\": \"clean\", \"output\": \"dirty\"},\n",
    "    {\"input\": \"heavy\", \"output\": \"light\"},\n",
    "    {\"input\": \"happy\", \"output\": \"angry\"},\n",
    "    {\"input\": \"高兴\", \"output\": \"悲伤\"},\n",
    "    {\"input\": \"愤怒\", \"output\": \"冷静\"},\n",
    "    {\"input\": \"晴天\", \"output\": \"雨天\"},\n",
    "    {\"input\": \"有风\", \"output\": \"平静\"},\n",
    "    {\"input\": \"轻松\", \"output\": \"紧张\"},\n",
    "    {\"input\": \"快\", \"output\": \"慢\"},\n",
    "    {\"input\": \"亲切\", \"output\": \"冷漠\"},\n",
    "    {\"input\": \"明亮\", \"output\": \"暗\"},\n",
    "    {\"input\": \"强\", \"output\": \"弱\"}\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"原词：{input}\\n反义：{output}\"\n",
    ")\n",
    "\n",
    "# 初始化OpenAIEmbeddings时显式传递密钥\n",
    "# embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "embeddings = OllamaEmbeddings(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"deepseek-r1:1.5b\"\n",
    ")\n",
    "\n",
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    examples,\n",
    "    embeddings,  # 使用已初始化的embeddings对象\n",
    "    FAISS,\n",
    "    k=2 # 选中的数量\n",
    ")\n",
    "\n",
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"请根据以下示例生成反义词：\",\n",
    "    suffix=\"原词是：{adjective}\\n反义词：\",\n",
    "    input_variables=[\"adjective\"]\n",
    ")\n",
    "\n",
    "print(mmr_prompt.format(adjective=\"worried\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:43:13.103373Z",
     "start_time": "2025-02-25T09:43:12.097195Z"
    }
   },
   "id": "6f35e717dc16af89",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,\n",
    "    embeddings,  # 使用已初始化的embeddings对象\n",
    "    Chroma,\n",
    "    k=2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:43:39.558558Z",
     "start_time": "2025-02-25T09:43:38.006499Z"
    }
   },
   "id": "cf1d2938e2cd9479",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题：谁的寿命更长，悟空还是如来\n",
      "\n",
      "            这里需要跟进问题吗： 是的。\n",
      "            跟进： 悟空在记载中，最后能够追溯到他有多大年龄？\n",
      "            中间答案： 五千岁\n",
      "            跟进： 如来呢？\n",
      "            中间答案： 佛祖在记载中，最后能够追溯到他多大年龄是一万岁。\n",
      "            所以最终答案是： 悟空五千年，如来一万岁，所以如来寿命更长。\n",
      "            \n",
      "\n",
      "问题：理论重要还是实践重要\n",
      "\n",
      "            这里需要跟进问题吗： 是的。\n",
      "            跟进：理论重要吗？\n",
      "            中间答案： 理论当然重要，理论为实践提供了指导和框架，帮助我们理解和预测事物的规律。\n",
      "            跟进： 实践重要吗？\n",
      "            中间答案： 实践则是理论的验证和应用，通过实践，我们能够检验和调整理论，发现新的问题和需求。\n",
      "            所以最终答案是： 都很重要。\n",
      "            \n",
      "\n",
      "问题：谁更厉害，孙悟空还是如来\n"
     ]
    }
   ],
   "source": [
    "example = [\n",
    "    {\n",
    "        \"question\": \"谁的寿命更长，悟空还是如来\",\n",
    "        \"answer\":\n",
    "            \"\"\"\n",
    "            这里需要跟进问题吗： 是的。\n",
    "            跟进： 悟空在记载中，最后能够追溯到他有多大年龄？\n",
    "            中间答案： 五千岁\n",
    "            跟进： 如来呢？\n",
    "            中间答案： 佛祖在记载中，最后能够追溯到他多大年龄是一万岁。\n",
    "            所以最终答案是： 悟空五千年，如来一万岁，所以如来寿命更长。\n",
    "            \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"理论重要还是实践重要\",\n",
    "        \"answer\":\n",
    "            \"\"\"\n",
    "            这里需要跟进问题吗： 是的。\n",
    "            跟进：理论重要吗？\n",
    "            中间答案： 理论当然重要，理论为实践提供了指导和框架，帮助我们理解和预测事物的规律。\n",
    "            跟进： 实践重要吗？\n",
    "            中间答案： 实践则是理论的验证和应用，通过实践，我们能够检验和调整理论，发现新的问题和需求。\n",
    "            所以最终答案是： 都很重要。\n",
    "            \"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"问题：{question}\\n{answer}\")\n",
    "# 做了个格式化\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,  # 这里需要是PromptTemplate(提示词模板)\n",
    "    examples=example,  # 这里需要是列表\n",
    "    suffix=\"问题：{input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "print(prompt.format(input=\"谁更厉害，孙悟空还是如来\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T09:44:17.780427Z",
     "start_time": "2025-02-25T09:44:17.775896Z"
    }
   },
   "id": "1110e9df9bed283a",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlangchain_core\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexample_selectors\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msemantic_similarity\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SemanticSimilarityExampleSelector\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# 使用 HuggingFace 的 Embedding 替代 OpenAIEmbeddings\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m example_selector \u001B[38;5;241m=\u001B[39m SemanticSimilarityExampleSelector\u001B[38;5;241m.\u001B[39mfrom_examples(\n\u001B[1;32m      7\u001B[0m     examples,\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;66;03m# HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"),  # 可以选择 HuggingFace 的任意模型\u001B[39;00m\n\u001B[1;32m      9\u001B[0m     OpenAIEmbeddings(openai_api_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msk-proj-Oj7EZ0hZpg-vKCQoMAJ6vcsi3mQMm38tvV34niNtD1CNYe1-VpEW33EWVddxdVz9DsXgxEPfUWT3BlbkFJlsbDggjnJchLOSvh-8Q7TLXketJDZRfIJbrLeDpsf41FvxipRPqjk4FzIR_lg4I91_0MRk7GUA\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     10\u001B[0m     Chroma,\n\u001B[1;32m     11\u001B[0m     k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     12\u001B[0m )\n\u001B[1;32m     14\u001B[0m question \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m寿命长短\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     15\u001B[0m selected_examples \u001B[38;5;241m=\u001B[39m example_selector\u001B[38;5;241m.\u001B[39mselect_examples({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m\"\u001B[39m: question})\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rag_test/lib/python3.12/site-packages/langchain_core/example_selectors/semantic_similarity.py:172\u001B[0m, in \u001B[0;36mSemanticSimilarityExampleSelector.from_examples\u001B[0;34m(cls, examples, embeddings, vectorstore_cls, k, input_keys, example_keys, vectorstore_kwargs, **vectorstore_cls_kwargs)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Create k-shot example selector using example list and embeddings.\u001B[39;00m\n\u001B[1;32m    153\u001B[0m \n\u001B[1;32m    154\u001B[0m \u001B[38;5;124;03mReshuffles examples dynamically based on query similarity.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;124;03m    The ExampleSelector instantiated, backed by a vector store.\u001B[39;00m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    171\u001B[0m string_examples \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_example_to_text(eg, input_keys) \u001B[38;5;28;01mfor\u001B[39;00m eg \u001B[38;5;129;01min\u001B[39;00m examples]\n\u001B[0;32m--> 172\u001B[0m vectorstore \u001B[38;5;241m=\u001B[39m vectorstore_cls\u001B[38;5;241m.\u001B[39mfrom_texts(\n\u001B[1;32m    173\u001B[0m     string_examples, embeddings, metadatas\u001B[38;5;241m=\u001B[39mexamples, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mvectorstore_cls_kwargs\n\u001B[1;32m    174\u001B[0m )\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\n\u001B[1;32m    176\u001B[0m     vectorstore\u001B[38;5;241m=\u001B[39mvectorstore,\n\u001B[1;32m    177\u001B[0m     k\u001B[38;5;241m=\u001B[39mk,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    180\u001B[0m     vectorstore_kwargs\u001B[38;5;241m=\u001B[39mvectorstore_kwargs,\n\u001B[1;32m    181\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rag_test/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:843\u001B[0m, in \u001B[0;36mChroma.from_texts\u001B[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001B[0m\n\u001B[1;32m    835\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mchromadb\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatch_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m create_batches\n\u001B[1;32m    837\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m create_batches(\n\u001B[1;32m    838\u001B[0m         api\u001B[38;5;241m=\u001B[39mchroma_collection\u001B[38;5;241m.\u001B[39m_client,  \u001B[38;5;66;03m# type: ignore[has-type]\u001B[39;00m\n\u001B[1;32m    839\u001B[0m         ids\u001B[38;5;241m=\u001B[39mids,\n\u001B[1;32m    840\u001B[0m         metadatas\u001B[38;5;241m=\u001B[39mmetadatas,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    841\u001B[0m         documents\u001B[38;5;241m=\u001B[39mtexts,\n\u001B[1;32m    842\u001B[0m     ):\n\u001B[0;32m--> 843\u001B[0m         chroma_collection\u001B[38;5;241m.\u001B[39madd_texts(\n\u001B[1;32m    844\u001B[0m             texts\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m batch[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;28;01melse\u001B[39;00m [],\n\u001B[1;32m    845\u001B[0m             metadatas\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m batch[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    846\u001B[0m             ids\u001B[38;5;241m=\u001B[39mbatch[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    847\u001B[0m         )\n\u001B[1;32m    848\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    849\u001B[0m     chroma_collection\u001B[38;5;241m.\u001B[39madd_texts(texts\u001B[38;5;241m=\u001B[39mtexts, metadatas\u001B[38;5;241m=\u001B[39mmetadatas, ids\u001B[38;5;241m=\u001B[39mids)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rag_test/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:277\u001B[0m, in \u001B[0;36mChroma.add_texts\u001B[0;34m(self, texts, metadatas, ids, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m texts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(texts)\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_embedding_function \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 277\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_embedding_function\u001B[38;5;241m.\u001B[39membed_documents(texts)\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m metadatas:\n\u001B[1;32m    279\u001B[0m     \u001B[38;5;66;03m# fill metadatas with empty dicts if somebody\u001B[39;00m\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;66;03m# did not specify metadata for all texts\u001B[39;00m\n\u001B[1;32m    281\u001B[0m     length_diff \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(texts) \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(metadatas)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rag_test/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:588\u001B[0m, in \u001B[0;36mOpenAIEmbeddings.embed_documents\u001B[0;34m(self, texts, chunk_size)\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[1;32m    586\u001B[0m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[1;32m    587\u001B[0m engine \u001B[38;5;241m=\u001B[39m cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeployment)\n\u001B[0;32m--> 588\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_len_safe_embeddings(texts, engine\u001B[38;5;241m=\u001B[39mengine)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rag_test/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:483\u001B[0m, in \u001B[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[0;34m(self, texts, engine, chunk_size)\u001B[0m\n\u001B[1;32m    481\u001B[0m batched_embeddings: List[List[\u001B[38;5;28mfloat\u001B[39m]] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n\u001B[0;32m--> 483\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[1;32m    484\u001B[0m         \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m=\u001B[39mtokens[i : i \u001B[38;5;241m+\u001B[39m _chunk_size], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_invocation_params\n\u001B[1;32m    485\u001B[0m     )\n\u001B[1;32m    486\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    487\u001B[0m         response \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mmodel_dump()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rag_test/lib/python3.12/site-packages/openai/resources/embeddings.py:128\u001B[0m, in \u001B[0;36mEmbeddings.create\u001B[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    122\u001B[0m             embedding\u001B[38;5;241m.\u001B[39membedding \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfrombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[1;32m    123\u001B[0m                 base64\u001B[38;5;241m.\u001B[39mb64decode(data), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    124\u001B[0m             )\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m    126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[0;32m--> 128\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/embeddings\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    130\u001B[0m     body\u001B[38;5;241m=\u001B[39mmaybe_transform(params, embedding_create_params\u001B[38;5;241m.\u001B[39mEmbeddingCreateParams),\n\u001B[1;32m    131\u001B[0m     options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[1;32m    132\u001B[0m         extra_headers\u001B[38;5;241m=\u001B[39mextra_headers,\n\u001B[1;32m    133\u001B[0m         extra_query\u001B[38;5;241m=\u001B[39mextra_query,\n\u001B[1;32m    134\u001B[0m         extra_body\u001B[38;5;241m=\u001B[39mextra_body,\n\u001B[1;32m    135\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    136\u001B[0m         post_parser\u001B[38;5;241m=\u001B[39mparser,\n\u001B[1;32m    137\u001B[0m     ),\n\u001B[1;32m    138\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mCreateEmbeddingResponse,\n\u001B[1;32m    139\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rag_test/lib/python3.12/site-packages/openai/_base_client.py:1290\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1276\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1277\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1278\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1285\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1286\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1287\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1288\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1289\u001B[0m     )\n\u001B[0;32m-> 1290\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rag_test/lib/python3.12/site-packages/openai/_base_client.py:967\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    964\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    965\u001B[0m     retries_taken \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 967\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[1;32m    968\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m    969\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m    970\u001B[0m     stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    971\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m    972\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39mretries_taken,\n\u001B[1;32m    973\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rag_test/lib/python3.12/site-packages/openai/_base_client.py:1056\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1054\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m remaining_retries \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_retry(err\u001B[38;5;241m.\u001B[39mresponse):\n\u001B[1;32m   1055\u001B[0m     err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m-> 1056\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retry_request(\n\u001B[1;32m   1057\u001B[0m         input_options,\n\u001B[1;32m   1058\u001B[0m         cast_to,\n\u001B[1;32m   1059\u001B[0m         retries_taken\u001B[38;5;241m=\u001B[39mretries_taken,\n\u001B[1;32m   1060\u001B[0m         response_headers\u001B[38;5;241m=\u001B[39merr\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[1;32m   1061\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m   1062\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m   1063\u001B[0m     )\n\u001B[1;32m   1065\u001B[0m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[1;32m   1066\u001B[0m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[1;32m   1067\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mis_closed:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rag_test/lib/python3.12/site-packages/openai/_base_client.py:1105\u001B[0m, in \u001B[0;36mSyncAPIClient._retry_request\u001B[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1101\u001B[0m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[1;32m   1102\u001B[0m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[1;32m   1103\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(timeout)\n\u001B[0;32m-> 1105\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[1;32m   1106\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   1107\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m   1108\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39mretries_taken \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   1109\u001B[0m     stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m   1110\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m   1111\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rag_test/lib/python3.12/site-packages/openai/_base_client.py:1056\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1054\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m remaining_retries \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_retry(err\u001B[38;5;241m.\u001B[39mresponse):\n\u001B[1;32m   1055\u001B[0m     err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m-> 1056\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retry_request(\n\u001B[1;32m   1057\u001B[0m         input_options,\n\u001B[1;32m   1058\u001B[0m         cast_to,\n\u001B[1;32m   1059\u001B[0m         retries_taken\u001B[38;5;241m=\u001B[39mretries_taken,\n\u001B[1;32m   1060\u001B[0m         response_headers\u001B[38;5;241m=\u001B[39merr\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[1;32m   1061\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m   1062\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m   1063\u001B[0m     )\n\u001B[1;32m   1065\u001B[0m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[1;32m   1066\u001B[0m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[1;32m   1067\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mis_closed:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rag_test/lib/python3.12/site-packages/openai/_base_client.py:1105\u001B[0m, in \u001B[0;36mSyncAPIClient._retry_request\u001B[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1101\u001B[0m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[1;32m   1102\u001B[0m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[1;32m   1103\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(timeout)\n\u001B[0;32m-> 1105\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[1;32m   1106\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   1107\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m   1108\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39mretries_taken \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   1109\u001B[0m     stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m   1110\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m   1111\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/rag_test/lib/python3.12/site-packages/openai/_base_client.py:1071\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1068\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m   1070\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1071\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1073\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[1;32m   1074\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m   1075\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1079\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39mretries_taken,\n\u001B[1;32m   1080\u001B[0m )\n",
      "\u001B[0;31mRateLimitError\u001B[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings  # 引入 HuggingFace Embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.example_selectors.semantic_similarity import SemanticSimilarityExampleSelector\n",
    "\n",
    "# 使用 HuggingFace 的 Embedding 替代 OpenAIEmbeddings\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"),  # 可以选择 HuggingFace 的任意模型\n",
    "    Chroma,\n",
    "    k=1\n",
    ")\n",
    "\n",
    "question = \"寿命长短\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "\n",
    "print(f\"最相似的示例:{question}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T11:05:13.498945Z",
     "start_time": "2025-02-25T11:05:09.460550Z"
    }
   },
   "id": "6911f54c461c2c11",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DeepSeekEmbeddings:\n",
    "    def __init__(self, model_name=\"deepseek-local\"):\n",
    "        self.model_name = model_name\n",
    "        self.embedding_dim = 1024  # 必须与实际模型维度一致\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [\n",
    "            [float(i%100)/100 for i in range(self.embedding_dim)]\n",
    "            for _ in texts\n",
    "        ]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T11:08:43.736297Z",
     "start_time": "2025-02-25T11:08:43.730454Z"
    }
   },
   "id": "7bfa6b6b7f4c579b",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最相似的示例:快乐星球在哪里\n",
      "input: 愤怒\n",
      "output: 冷静\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "class DeepSeekEmbeddings:\n",
    "    def __init__(self, model_name=\"deepseek-local\"):\n",
    "        self.model_name = model_name\n",
    "        self.embedding_dim = 1536  # 必须与实际模型维度一致\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [\n",
    "            [float(i%100)/100 for i in range(self.embedding_dim)]\n",
    "            for _ in texts\n",
    "        ]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,\n",
    "    DeepSeekEmbeddings(),\n",
    "    Chroma,\n",
    "    k=1\n",
    ")\n",
    "\n",
    "question = \"快乐星球在哪里\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "\n",
    "print(f\"最相似的示例:{question}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\".join([f\"{k}: {v}\" for k, v in example.items()]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-25T11:10:14.146324Z",
     "start_time": "2025-02-25T11:10:14.103702Z"
    }
   },
   "id": "5f28da5883c760f6",
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
